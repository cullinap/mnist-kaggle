{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numbers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples 42000\n",
      "Number of training pixels 784\n",
      "Number of classes 10\n",
      "Number of test samples 28000\n",
      "Number of test pixels 784\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(os.getcwd(),'all/train.csv')\n",
    "test_path  = os.path.join(os.getcwd(),'all/test.csv')\n",
    "train = pd.read_csv(train_path)\n",
    "test  = pd.read_csv(test_path)\n",
    "\n",
    "n_train = len(train)\n",
    "n_pixels = len(train.columns) - 1\n",
    "n_class = len(set(train['label']))\n",
    "n_test = len(test)\n",
    "n_test_pixels = len(test.columns)\n",
    "\n",
    "print('Number of training samples {0}' .format(n_train))\n",
    "print('Number of training pixels {0}' .format(n_pixels))\n",
    "print('Number of classes {0}' .format(n_class))\n",
    "print('Number of test samples {0}'. format(n_test))\n",
    "print('Number of test pixels {0}'. format(n_test_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.loc[:,'pixel0':].values.reshape(train.shape[0],1,28,28)\\\n",
    "                    .astype('float32')\n",
    "y_train = train.loc[:,'label']\n",
    "x_test  = test.values.reshape(test.shape[0],1,28,28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1162a2e80>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162956a0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 69\n",
    "print(y_train[image_index])\n",
    "plt.imshow(x_train[image_index,0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_data(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=transforms.Compose([transforms.ToPILImage(),\\\n",
    "                        transforms.ToTensor(),transforms.Normalize(mean=(0.5,), std=(0.5,))])):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if len(df.columns) == n_pixels:\n",
    "            #test data\n",
    "            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = None\n",
    "        else:\n",
    "            #training data\n",
    "            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = torch.from_numpy(df.iloc[:,0].values)\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.transform(self.X[idx]), self.y[idx]\n",
    "        else:\n",
    "            return self.transform(self.X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation(object):\n",
    "    \"\"\"\n",
    "    https://github.com/pytorch/vision/tree/master/torchvision/transforms\n",
    "    Rotate the image by angle.\n",
    "    Args:\n",
    "        degrees (sequence or float or int): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees).\n",
    "        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "        expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "        center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees, resample=False, expand=False, center=None):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n",
    "            self.degrees = degrees\n",
    "\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        angle = np.random.uniform(degrees[0], degrees[1])\n",
    "\n",
    "        return angle\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "            img (PIL Image): Image to be rotated.\n",
    "        Returns:\n",
    "            PIL Image: Rotated image.\n",
    "        \"\"\"\n",
    "        \n",
    "        def rotate(img, angle, resample=False, expand=False, center=None):\n",
    "            \"\"\"Rotate the image by angle and then (optionally) translate it by (n_columns, n_rows)\n",
    "            Args:\n",
    "            img (PIL Image): PIL Image to be rotated.\n",
    "            angle ({float, int}): In degrees degrees counter clockwise order.\n",
    "            resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "            expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output image to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "            center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "            \"\"\"\n",
    "                \n",
    "            return img.rotate(angle, resample, expand, center)\n",
    "\n",
    "        angle = self.get_params(self.degrees)\n",
    "\n",
    "        return rotate(img, angle, self.resample, self.expand, self.center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomShift(object):\n",
    "    def __init__(self, shift):\n",
    "        self.shift = shift\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_params(shift):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        hshift, vshift = np.random.uniform(-shift, shift, size=2)\n",
    "\n",
    "        return hshift, vshift \n",
    "    def __call__(self, img):\n",
    "        hshift, vshift = self.get_params(self.shift)\n",
    "        \n",
    "        return img.transform(img.size, Image.AFFINE, (1,0,hshift,0,1,vshift), resample=Image.BICUBIC, fill=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = MNIST_data(train_path, transform= transforms.Compose(\n",
    "                            [transforms.ToPILImage(), RandomRotation(degrees=20), RandomShift(3),\n",
    "                             transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))]))\n",
    "test_dataset = MNIST_data(test_path)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "          \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "          \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "          \n",
    "        for m in self.features.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        for m in self.classifier.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickcullinane/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Dropout(p=0.5)\n",
       "    (5): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Dropout(p=0.5)\n",
       "    (9): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Net()\n",
    "optimizer = optim.Adam(network.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    exp_lr_scheduler.step()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    network.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in data_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        output = network(data)\n",
    "        \n",
    "        loss += F.cross_entropy(output, target, size_average=False).data[0]\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "    \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-c1f0767da33b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-a76d8481c568>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n\u001b[1;32m     17\u001b[0m                 \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 100. * (batch_idx + 1) / len(train_loader), loss.data[0]))\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
